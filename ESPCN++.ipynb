{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from PIL import Image \n",
    "import numpy as np \n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_dir = r\"C:/Users/91995/Downloads/div2k/DIV2K_valid_HR/DIV2K_valid_HR\"\n",
    "hr_dir = r\"C:/Users/91995/Downloads/div2k/DIV2K_valid_HR/DIV2K_valid_HR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_size = 2\n",
    "HR_SIZE = (400, 400)\n",
    "LR_SIZE = (200, 200)\n",
    "patch_size = 10  \n",
    "stride = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import cv2\n",
    "def load_images_from_dir(directory, target_size=None):\n",
    "    images = []\n",
    "    valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        # Check if file has valid image extension\n",
    "        if os.path.splitext(filename.lower())[1] in valid_extensions:\n",
    "            img_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img_array = np.array(img, dtype=np.uint8)\n",
    "                \n",
    "                # Ensure image is 3D (height, width, channels)\n",
    "                if len(img_array.shape) != 3:\n",
    "                    continue\n",
    "                    \n",
    "                img_tensor = tf.convert_to_tensor(img_array, dtype=tf.uint8)\n",
    "                img_tensor = tf.cast(img_tensor, tf.float32) / 255.0\n",
    "                \n",
    "                if target_size:\n",
    "                    img_tensor = tf.image.resize(img_tensor, target_size, method='area')\n",
    "                \n",
    "                img_array = tf.cast(img_tensor * 255.0, tf.uint8).numpy()\n",
    "                img_yuv = cv2.cvtColor(img_array, cv2.COLOR_RGB2YUV)\n",
    "                images.append(img_yuv)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    if not images:\n",
    "        raise ValueError(f\"No valid images found in directory: {directory}\")\n",
    "        \n",
    "    return np.array(images, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_images = load_images_from_dir(lr_dir, target_size=(200, 200))  \n",
    "hr_images = load_images_from_dir(hr_dir, target_size=(400, 400)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(input, input_size, upscale_factor):\n",
    "    input = tf.cast(input, tf.float32) / 255.0\n",
    "    lr_image = tf.image.resize(input, [input_size, input_size], method=\"area\")\n",
    "    return lr_image\n",
    "\n",
    "def process_target(input):\n",
    "    input = tf.cast(input, tf.float32) / 255.0\n",
    "    hr_image = tf.image.resize(input, HR_SIZE, method='bilinear')\n",
    "    return hr_image\n",
    "\n",
    "def preprocess_image(data):\n",
    "    hr_image = process_target(data)\n",
    "    lr_image = process_input(data, LR_SIZE[0], scale_size)\n",
    "    return lr_image, hr_image\n",
    "\n",
    "def create_patches(image, patch_size, stride):\n",
    "    # Ensure input is 4D (batch, height, width, channels)\n",
    "    if len(image.shape) == 3:\n",
    "        image = tf.expand_dims(image, 0)\n",
    "    \n",
    "    patches = tf.image.extract_patches(\n",
    "        images=image,\n",
    "        sizes=[1, patch_size, patch_size, 1],\n",
    "        strides=[1, stride, stride, 1],\n",
    "        rates=[1, 1, 1, 1],\n",
    "        padding='VALID'\n",
    "    )\n",
    "    \n",
    "    # Reshape patches to (num_patches, patch_size, patch_size, channels)\n",
    "    patches = tf.reshape(patches, [-1, patch_size, patch_size, image.shape[-1]])\n",
    "    return patches\n",
    "\n",
    "def apply_preprocessing_on_local_data(lr_images, hr_images):\n",
    "    if len(lr_images) != len(hr_images):\n",
    "        raise ValueError(\"Number of LR and HR images must match\")\n",
    "    \n",
    "    lr_patches_list = []\n",
    "    hr_patches_list = []\n",
    "    \n",
    "    for i, (lr_image, hr_image) in enumerate(zip(lr_images, hr_images)):\n",
    "        try:\n",
    "            # Preprocess images\n",
    "            lr_processed, hr_processed = preprocess_image(hr_image)\n",
    "            \n",
    "            # Create patches\n",
    "            lr_patches = create_patches(lr_processed, patch_size, stride)\n",
    "            hr_patches = create_patches(hr_processed, patch_size * scale_size, stride * scale_size)\n",
    "            \n",
    "            # Verify patches were created successfully\n",
    "            if tf.shape(lr_patches)[0] > 0 and tf.shape(hr_patches)[0] > 0:\n",
    "                lr_patches_list.append(lr_patches)\n",
    "                hr_patches_list.append(hr_patches)\n",
    "            else:\n",
    "                print(f\"Warning: No patches created for image {i}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {i}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not lr_patches_list or not hr_patches_list:\n",
    "        raise ValueError(\"No valid patches were created from any images\")\n",
    "    \n",
    "    # Concatenate all patches\n",
    "    lr_patches = tf.concat(lr_patches_list, axis=0)\n",
    "    hr_patches = tf.concat(hr_patches_list, axis=0)\n",
    "    \n",
    "    return lr_patches, hr_patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 LR images and 100 HR images\n",
      "Created 152100 LR patches and 152100 HR patches\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load images\n",
    "    lr_images = load_images_from_dir(lr_dir, target_size=LR_SIZE)\n",
    "    hr_images = load_images_from_dir(hr_dir, target_size=HR_SIZE)\n",
    "    \n",
    "    print(f\"Loaded {len(lr_images)} LR images and {len(hr_images)} HR images\")\n",
    "    \n",
    "    # Create patches\n",
    "    lr_patches, hr_patches = apply_preprocessing_on_local_data(lr_images, hr_images)\n",
    "    \n",
    "    print(f\"Created {tf.shape(lr_patches)[0]} LR patches and {tf.shape(hr_patches)[0]} HR patches\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in main processing: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_patches, hr_patches = apply_preprocessing_on_local_data(lr_images, hr_images)\n",
    "#print(\"Low-res patches shape:\", lr_patches.shape)\n",
    "#print(\"High-res patches shape:\", hr_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr(y_true, y_pred):\n",
    "    max_pixel = 1.0\n",
    "    return tf.image.psnr(y_true, y_pred, max_val=max_pixel)\n",
    "\n",
    "class DepthToSpace(tf.keras.layers.Layer):\n",
    "    def __init__(self, scale, **kwargs):\n",
    "        super(DepthToSpace, self).__init__(**kwargs)\n",
    "        self.scale = scale\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.depth_to_space(inputs, self.scale)\n",
    "\n",
    "def leaky_relu_activation(x):\n",
    "    leaky_relu_out = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    linear_out = x\n",
    "    return leaky_relu_out + linear_out\n",
    "\n",
    "class SinglePixelAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SinglePixelAttention, self).__init__(**kwargs)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        mean_values = tf.reduce_mean(inputs, axis=-1)\n",
    "        max_indices = tf.argmax(mean_values, axis=-1)\n",
    "        attention_mask = tf.one_hot(max_indices, depth=tf.shape(inputs)[1])\n",
    "        attention_mask = tf.expand_dims(attention_mask, axis=-1)\n",
    "        attention_output = inputs * attention_mask\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_espcn_model_with_skip_connections(input_shape, scale_size):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    conv1 = tf.keras.layers.Conv2D(16, 5, padding='same', kernel_initializer=tf.keras.initializers.HeNormal())(inputs)\n",
    "    conv1 = tf.keras.layers.LeakyReLU(alpha=0.2)(conv1)\n",
    "    conv2 = tf.keras.layers.Conv2D(16, 3, padding='same', kernel_initializer=tf.keras.initializers.HeNormal())(conv1)\n",
    "    conv2 = tf.keras.layers.LeakyReLU(alpha=0.2)(conv2)\n",
    "    skip1 = tf.keras.layers.Add()([conv1, conv2])\n",
    "    conv3 = tf.keras.layers.Conv2D(16, 2, padding='same', kernel_initializer='orthogonal')(skip1)\n",
    "    conv3 = tf.keras.layers.Lambda(leaky_relu_activation)(conv3)\n",
    "    conv3 = SinglePixelAttention()(conv3)\n",
    "    skip2 = tf.keras.layers.Add()([skip1, conv3])\n",
    "    conv4 = tf.keras.layers.Conv2D(3 * (scale_size ** 2), 3, padding='same', kernel_initializer='orthogonal')(skip2)\n",
    "    conv4 = tf.keras.layers.LeakyReLU(alpha=0.2)(conv4)\n",
    "    outputs = DepthToSpace(scale_size)(conv4)\n",
    "    model = tf.keras.models.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (patch_size, patch_size, 3)\n",
    "espcn_model_with_skip_and_attention = build_espcn_model_with_skip_connections(input_shape, scale_size)\n",
    "espcn_model_with_skip_and_attention.compile(optimizer='adam', loss='mse', metrics=[psnr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m4754/4754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 3ms/step - loss: 0.0077 - psnr: 27.3023\n",
      "Epoch 2/20\n",
      "\u001b[1m4754/4754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 8.0837e-04 - psnr: 34.3721\n",
      "Epoch 3/20\n",
      "\u001b[1m4754/4754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 7.5276e-04 - psnr: 35.1159\n",
      "Epoch 4/20\n",
      "\u001b[1m4754/4754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 3ms/step - loss: 7.3351e-04 - psnr: 35.4071\n",
      "Epoch 5/20\n",
      "\u001b[1m4754/4754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 7.2575e-04 - psnr: 35.5280\n",
      "Epoch 6/20\n",
      "\u001b[1m4754/4754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 7.1931e-04 - psnr: 35.6660\n",
      "Epoch 7/20\n",
      "\u001b[1m4754/4754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - loss: 7.1558e-04 - psnr: 35.7579\n",
      "Epoch 8/20\n",
      "\u001b[1m4754/4754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 3ms/step - loss: 7.1535e-04 - psnr: 35.8222\n",
      "Epoch 9/20\n",
      "\u001b[1m4754/4754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - loss: 7.1441e-04 - psnr: 35.8643\n",
      "Epoch 10/20\n",
      "\u001b[1m4754/4754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 7.0926e-04 - psnr: 35.9473\n",
      "Epoch 11/20\n",
      "\u001b[1m4754/4754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 4ms/step - loss: 7.1049e-04 - psnr: 35.9013\n",
      "Epoch 12/20\n",
      "\u001b[1m4754/4754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4ms/step - loss: 7.1022e-04 - psnr: 35.9253\n",
      "Epoch 13/20\n",
      "\u001b[1m4754/4754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 3ms/step - loss: 7.0739e-04 - psnr: 35.9182\n",
      "Epoch 14/20\n",
      "\u001b[1m4754/4754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 3ms/step - loss: 7.0411e-04 - psnr: 35.9885\n",
      "Epoch 15/20\n",
      "\u001b[1m4754/4754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 3ms/step - loss: 7.0052e-04 - psnr: 35.9989\n",
      "Epoch 16/20\n",
      "\u001b[1m4754/4754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 3ms/step - loss: 7.0328e-04 - psnr: 35.9660\n",
      "Epoch 17/20\n",
      "\u001b[1m4754/4754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 3ms/step - loss: 6.9542e-04 - psnr: 35.9802\n",
      "Epoch 18/20\n",
      "\u001b[1m4754/4754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4ms/step - loss: 6.9888e-04 - psnr: 35.9625\n",
      "Epoch 19/20\n",
      "\u001b[1m4754/4754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 3ms/step - loss: 6.9541e-04 - psnr: 36.0314\n",
      "Epoch 20/20\n",
      "\u001b[1m4754/4754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4ms/step - loss: 6.9369e-04 - psnr: 36.0176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x25c383aeed0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "espcn_model_with_skip_and_attention.fit(lr_patches, hr_patches, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = r\"C:/Users/91995/Downloads/Set5/Set5/image_SRF_2/img_005_SRF_2_LR.png\"\n",
    "output_file = r\"C:/Users/91995/Downloads/Set5/Set5/image_SRF_2/Gal\"\n",
    "gt = r\"C:/Users/91995/Downloads/Set5/Set5/image_SRF_2/img_005_SRF_2_HR.png\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# After training your model:\\ninference_model = ESPCNInference(scale_size=2, patch_size=10)  # Changed scale_size to 2\\n\\n# Transfer weights from trained model\\ninference_model.model.set_weights(espcn_model_with_skip_and_attention.get_weights())\\n\\n# Save the model if needed\\ninference_model.save_model('espcn_model.h5')\\n\\n# Upscale a single image\\nupscaled_image = inference_model.upscale_image('input.jpg', 'output.jpg')\\n\\n# Or upscale all images in a directory\\ninference_model.upscale_directory('input_dir', 'output_dir')\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "class ESPCNInference:\n",
    "    def __init__(self, model_path=None, scale_size=2, patch_size=10):\n",
    "        self.scale_size = scale_size  \n",
    "        self.patch_size = patch_size\n",
    "        self.model = None\n",
    "        if model_path:\n",
    "            self.load_model(model_path)\n",
    "        else:\n",
    "\n",
    "            input_shape = (patch_size, patch_size, 3)\n",
    "            self.model = build_espcn_model_with_skip_connections(input_shape, scale_size)\n",
    "    \n",
    "    def load_model(self, model_path):\n",
    "\n",
    "        try:\n",
    "            self.model = tf.keras.models.load_model(\n",
    "                model_path,\n",
    "                custom_objects={\n",
    "                    'DepthToSpace': DepthToSpace,\n",
    "                    'SinglePixelAttention': SinglePixelAttention,\n",
    "                    'psnr': psnr\n",
    "                }\n",
    "            )\n",
    "            print(\"Model loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def save_model(self, model_path):\n",
    "\n",
    "        self.model.save(model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    def preprocess_image(self, image_path):\n",
    "        try:\n",
    "            \n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "            img_array = np.array(img, dtype=np.uint8)\n",
    "            img_yuv = cv2.cvtColor(img_array, cv2.COLOR_RGB2YUV)\n",
    "            \n",
    "            #\n",
    "            img_yuv = img_yuv.astype(np.float32) / 255.0\n",
    "            \n",
    "            \n",
    "            img_yuv = np.expand_dims(img_yuv, axis=0)\n",
    "            \n",
    "            return img_yuv, img_array.shape[:2]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error preprocessing image: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def create_patches(self, image, include_padding=True):\n",
    "\n",
    "        h, w = image.shape[1:3]\n",
    "        \n",
    "        if include_padding:\n",
    "            \n",
    "            pad_h = (self.patch_size - h % self.patch_size) % self.patch_size\n",
    "            pad_w = (self.patch_size - w % self.patch_size) % self.patch_size\n",
    "            \n",
    "            \n",
    "            if pad_h > 0 or pad_w > 0:\n",
    "                image = tf.pad(image, [[0, 0], [0, pad_h], [0, pad_w], [0, 0]], mode='REFLECT')\n",
    "        \n",
    "        \n",
    "        patches = tf.image.extract_patches(\n",
    "            images=image,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding='VALID'\n",
    "        )\n",
    "        \n",
    "        \n",
    "        patches = tf.reshape(patches, [-1, self.patch_size, self.patch_size, 3])\n",
    "        return patches, image.shape[1:3]\n",
    "    \n",
    "    def reconstruct_image(self, patches, original_size, padded_size):\n",
    "\n",
    "        \n",
    "        h, w = padded_size\n",
    "        patch_size_hr = self.patch_size * self.scale_size  \n",
    "        patches_per_row = w // self.patch_size\n",
    "        patches_per_col = h // self.patch_size\n",
    "        \n",
    "        \n",
    "        patches = tf.reshape(patches, [patches_per_col, patches_per_row, \n",
    "                                     patch_size_hr, patch_size_hr, 3])\n",
    "        \n",
    "        \n",
    "        image = tf.transpose(patches, [0, 2, 1, 3, 4])\n",
    "        image = tf.reshape(image, [1, h * self.scale_size, w * self.scale_size, 3])\n",
    "        \n",
    "        \n",
    "        orig_h, orig_w = original_size\n",
    "        image = image[:, :orig_h * self.scale_size, :orig_w * self.scale_size, :]\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def postprocess_image(self, image):\n",
    "\n",
    "        image = tf.clip_by_value(image, 0, 1)\n",
    "        image = tf.round(image * 255.0)\n",
    "        image = tf.cast(image, tf.uint8)\n",
    "        image = cv2.cvtColor(image[0].numpy(), cv2.COLOR_YUV2RGB)\n",
    "        return image\n",
    "    \n",
    "    def upscale_image(self, image_path, output_path=None):\n",
    "      \n",
    "        try:\n",
    "\n",
    "            img_yuv, original_size = self.preprocess_image(image_path)\n",
    "            \n",
    "            patches, padded_size = self.create_patches(img_yuv)\n",
    "            \n",
    "           \n",
    "            sr_patches = self.model.predict(patches, verbose=0)\n",
    "            \n",
    "           \n",
    "            sr_image = self.reconstruct_image(sr_patches, original_size, padded_size)\n",
    "            \n",
    "           \n",
    "            final_image = self.postprocess_image(sr_image)\n",
    "            \n",
    "            if output_path:\n",
    "                Image.fromarray(final_image).save(output_path)\n",
    "                print(f\"Saved super-resolved image to {output_path}\")\n",
    "            \n",
    "            return final_image\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during upscaling: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def upscale_directory(self, input_dir, output_dir):\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        for filename in os.listdir(input_dir):\n",
    "            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "                input_path = os.path.join(input_dir, filename)\n",
    "                output_path = os.path.join(output_dir, f\"sr_{filename}\")\n",
    "                \n",
    "                try:\n",
    "                    self.upscale_image(input_path, output_path)\n",
    "                    print(f\"Processed {filename}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filename}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# After training your model:\n",
    "inference_model = ESPCNInference(scale_size=2, patch_size=10)  # Changed scale_size to 2\n",
    "\n",
    "# Transfer weights from trained model\n",
    "inference_model.model.set_weights(espcn_model_with_skip_and_attention.get_weights())\n",
    "\n",
    "# Save the model if needed\n",
    "inference_model.save_model('espcn_model.h5')\n",
    "\n",
    "# Upscale a single image\n",
    "upscaled_image = inference_model.upscale_image('input.jpg', 'output.jpg')\n",
    "\n",
    "# Or upscale all images in a directory\n",
    "inference_model.upscale_directory('input_dir', 'output_dir')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during upscaling: unknown file extension: \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unknown file extension: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\91995\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:2416\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2416\u001b[0m     \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m EXTENSION[ext]\n\u001b[0;32m   2417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyError\u001b[0m: ''",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m inference_model \u001b[38;5;241m=\u001b[39m ESPCNInference(scale_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m upscaled_image \u001b[38;5;241m=\u001b[39m inference_model\u001b[38;5;241m.\u001b[39mupscale_image(input_file, output_file)\n",
      "Cell \u001b[1;32mIn[34], line 133\u001b[0m, in \u001b[0;36mESPCNInference.upscale_image\u001b[1;34m(self, image_path, output_path)\u001b[0m\n\u001b[0;32m    130\u001b[0m final_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess_image(sr_image)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_path:\n\u001b[1;32m--> 133\u001b[0m     Image\u001b[38;5;241m.\u001b[39mfromarray(final_image)\u001b[38;5;241m.\u001b[39msave(output_path)\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved super-resolved image to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_image\n",
      "File \u001b[1;32mc:\\Users\\91995\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:2419\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2417\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2418\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown file extension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 2419\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   2421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SAVE:\n\u001b[0;32m   2422\u001b[0m     init()\n",
      "\u001b[1;31mValueError\u001b[0m: unknown file extension: "
     ]
    }
   ],
   "source": [
    "inference_model = ESPCNInference(scale_size=4, patch_size=10)\n",
    "upscaled_image = inference_model.upscale_image(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Initialize model\\ninference_model = ESPCNInference(scale_size=2, patch_size=10)\\n\\n# Load trained weights\\ninference_model.load_model('espcn_model.h5')\\n\\n# Upscale a single image and calculate metrics\\nupscaled_image, metrics = inference_model.upscale_image(\\n    'input.jpg',\\n    'output',  # Will automatically add .png extension\\n    'ground_truth_hr.jpg'  # Optional HR image for quality assessment\\n)\\n\\n# Upscale all images in a directory and calculate metrics\\ninference_model.upscale_directory(\\n    'input_dir',\\n    'output_dir',\\n    'hr_dir'  # Optional directory containing HR images\\n)\\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "class ESPCNInference:\n",
    "    def __init__(self, model_path=None, scale_size=2, patch_size=10):\n",
    "        self.scale_size = scale_size\n",
    "        self.patch_size = patch_size\n",
    "        self.model = None\n",
    "        if model_path:\n",
    "            self.load_model(model_path)\n",
    "        else:\n",
    "            input_shape = (patch_size, patch_size, 3)\n",
    "            self.model = build_espcn_model_with_skip_connections(input_shape, scale_size)\n",
    "    \n",
    "    def load_model(self, model_path):\n",
    "        \"\"\"Load trained model weights\"\"\"\n",
    "        try:\n",
    "            self.model = tf.keras.models.load_model(\n",
    "                model_path,\n",
    "                custom_objects={\n",
    "                    'DepthToSpace': DepthToSpace,\n",
    "                    'SinglePixelAttention': SinglePixelAttention,\n",
    "                    'psnr': psnr\n",
    "                }\n",
    "            )\n",
    "            print(\"Model loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def save_model(self, model_path):\n",
    "        \"\"\"Save the trained model\"\"\"\n",
    "        self.model.save(model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    def calculate_psnr(self, img1, img2):\n",
    "        \"\"\"Calculate PSNR between two images\"\"\"\n",
    "        # Convert to float32\n",
    "        img1 = img1.astype(np.float32)\n",
    "        img2 = img2.astype(np.float32)\n",
    "        \n",
    "        # Calculate MSE\n",
    "        mse = np.mean((img1 - img2) ** 2)\n",
    "        if mse == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        # Calculate PSNR\n",
    "        max_pixel = 255.0\n",
    "        psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
    "        return psnr\n",
    "    \n",
    "    def calculate_ssim(self, img1, img2):\n",
    "        \"\"\"Calculate SSIM between two images\"\"\"\n",
    "        C1 = (0.01 * 255)**2\n",
    "        C2 = (0.03 * 255)**2\n",
    "        \n",
    "        img1 = img1.astype(np.float32)\n",
    "        img2 = img2.astype(np.float32)\n",
    "        \n",
    "        kernel = cv2.getGaussianKernel(11, 1.5)\n",
    "        window = np.outer(kernel, kernel.transpose())\n",
    "        \n",
    "        mu1 = cv2.filter2D(img1, -1, window)[5:-5, 5:-5]\n",
    "        mu2 = cv2.filter2D(img2, -1, window)[5:-5, 5:-5]\n",
    "        mu1_sq = mu1**2\n",
    "        mu2_sq = mu2**2\n",
    "        mu1_mu2 = mu1 * mu2\n",
    "        \n",
    "        sigma1_sq = cv2.filter2D(img1**2, -1, window)[5:-5, 5:-5] - mu1_sq\n",
    "        sigma2_sq = cv2.filter2D(img2**2, -1, window)[5:-5, 5:-5] - mu2_sq\n",
    "        sigma12 = cv2.filter2D(img1 * img2, -1, window)[5:-5, 5:-5] - mu1_mu2\n",
    "        \n",
    "        ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / \\\n",
    "                   ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "        return np.mean(ssim_map)\n",
    "    \n",
    "    def evaluate_image_quality(self, sr_image, hr_image):\n",
    "        \"\"\"Calculate quality metrics between super-resolved and high-resolution images\"\"\"\n",
    "        # Ensure images are the same size\n",
    "        if sr_image.shape != hr_image.shape:\n",
    "            hr_image = cv2.resize(hr_image, (sr_image.shape[1], sr_image.shape[0]))\n",
    "        \n",
    "        # Calculate metrics for each channel\n",
    "        psnr_values = []\n",
    "        ssim_values = []\n",
    "        \n",
    "        for c in range(3):  # For each RGB channel\n",
    "            psnr_values.append(self.calculate_psnr(sr_image[:,:,c], hr_image[:,:,c]))\n",
    "            ssim_values.append(self.calculate_ssim(sr_image[:,:,c], hr_image[:,:,c]))\n",
    "        \n",
    "        metrics = {\n",
    "            'psnr': {\n",
    "                'average': np.mean(psnr_values),\n",
    "                'per_channel': psnr_values\n",
    "            },\n",
    "            'ssim': {\n",
    "                'average': np.mean(ssim_values),\n",
    "                'per_channel': ssim_values\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def preprocess_image(self, image_path):\n",
    "        \"\"\"Preprocess a single image for inference\"\"\"\n",
    "        try:\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "            img_array = np.array(img, dtype=np.uint8)\n",
    "            img_yuv = cv2.cvtColor(img_array, cv2.COLOR_RGB2YUV)\n",
    "            img_yuv = img_yuv.astype(np.float32) / 255.0\n",
    "            img_yuv = np.expand_dims(img_yuv, axis=0)\n",
    "            return img_yuv, img_array.shape[:2]\n",
    "        except Exception as e:\n",
    "            print(f\"Error preprocessing image: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def create_patches(self, image, include_padding=True):\n",
    "        \"\"\"Create patches from the input image with optional padding\"\"\"\n",
    "        h, w = image.shape[1:3]\n",
    "        \n",
    "        if include_padding:\n",
    "            pad_h = (self.patch_size - h % self.patch_size) % self.patch_size\n",
    "            pad_w = (self.patch_size - w % self.patch_size) % self.patch_size\n",
    "            \n",
    "            if pad_h > 0 or pad_w > 0:\n",
    "                image = tf.pad(image, [[0, 0], [0, pad_h], [0, pad_w], [0, 0]], mode='REFLECT')\n",
    "        \n",
    "        patches = tf.image.extract_patches(\n",
    "            images=image,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding='VALID'\n",
    "        )\n",
    "        \n",
    "        patches = tf.reshape(patches, [-1, self.patch_size, self.patch_size, 3])\n",
    "        return patches, image.shape[1:3]\n",
    "    \n",
    "    def reconstruct_image(self, patches, original_size, padded_size):\n",
    "        \"\"\"Reconstruct the full image from patches\"\"\"\n",
    "        h, w = padded_size\n",
    "        patch_size_hr = self.patch_size * self.scale_size\n",
    "        patches_per_row = w // self.patch_size\n",
    "        patches_per_col = h // self.patch_size\n",
    "        \n",
    "        patches = tf.reshape(patches, [patches_per_col, patches_per_row, \n",
    "                                     patch_size_hr, patch_size_hr, 3])\n",
    "        \n",
    "        image = tf.transpose(patches, [0, 2, 1, 3, 4])\n",
    "        image = tf.reshape(image, [1, h * self.scale_size, w * self.scale_size, 3])\n",
    "        \n",
    "        orig_h, orig_w = original_size\n",
    "        image = image[:, :orig_h * self.scale_size, :orig_w * self.scale_size, :]\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def postprocess_image(self, image):\n",
    "        \"\"\"Convert image back to RGB and uint8 format\"\"\"\n",
    "        image = tf.clip_by_value(image, 0, 1)\n",
    "        image = tf.round(image * 255.0)\n",
    "        image = tf.cast(image, tf.uint8)\n",
    "        image = cv2.cvtColor(image[0].numpy(), cv2.COLOR_YUV2RGB)\n",
    "        return image\n",
    "    \n",
    "    def upscale_image(self, image_path, output_path=None, hr_image_path=None):\n",
    "        \"\"\"Upscale a single image and optionally calculate quality metrics\"\"\"\n",
    "        try:\n",
    "            # Preprocess\n",
    "            img_yuv, original_size = self.preprocess_image(image_path)\n",
    "            \n",
    "            # Create patches\n",
    "            patches, padded_size = self.create_patches(img_yuv)\n",
    "            \n",
    "            # Process patches\n",
    "            sr_patches = self.model.predict(patches, verbose=0)\n",
    "            \n",
    "            # Reconstruct image\n",
    "            sr_image = self.reconstruct_image(sr_patches, original_size, padded_size)\n",
    "            \n",
    "            # Postprocess\n",
    "            final_image = self.postprocess_image(sr_image)\n",
    "            \n",
    "            # Calculate metrics if HR image is provided\n",
    "            metrics = None\n",
    "            if hr_image_path:\n",
    "                hr_image = np.array(Image.open(hr_image_path).convert('RGB'))\n",
    "                metrics = self.evaluate_image_quality(final_image, hr_image)\n",
    "                print(\"\\nImage Quality Metrics:\")\n",
    "                print(f\"PSNR: {metrics['psnr']['average']:.2f} dB\")\n",
    "                print(f\"SSIM: {metrics['ssim']['average']:.4f}\")\n",
    "            \n",
    "            if output_path:\n",
    "                Image.fromarray(final_image).save(output_path)\n",
    "                print(f\"Saved super-resolved image to {output_path}\")\n",
    "            \n",
    "            return final_image, metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during upscaling: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _ensure_valid_extension(self, filepath):\n",
    "        \"\"\"Ensure the filepath has a valid image extension\"\"\"\n",
    "        valid_extensions = {'.png', '.jpg', '.jpeg', '.bmp', '.tiff'}\n",
    "        _, ext = os.path.splitext(filepath)\n",
    "        ext = ext.lower()\n",
    "        \n",
    "        if not ext:\n",
    "            # If no extension, default to PNG\n",
    "            filepath = f\"{filepath}.png\"\n",
    "        elif ext not in valid_extensions:\n",
    "            # If invalid extension, append PNG\n",
    "            filepath = f\"{filepath}.png\"\n",
    "            \n",
    "        return filepath\n",
    "\n",
    "    def upscale_image(self, image_path, output_path=None, hr_image_path=None):\n",
    "        \"\"\"Upscale a single image and optionally calculate quality metrics\"\"\"\n",
    "        try:\n",
    "            # Preprocess\n",
    "            img_yuv, original_size = self.preprocess_image(image_path)\n",
    "            \n",
    "            # Create patches\n",
    "            patches, padded_size = self.create_patches(img_yuv)\n",
    "            \n",
    "            # Process patches\n",
    "            sr_patches = self.model.predict(patches, verbose=0)\n",
    "            \n",
    "            # Reconstruct image\n",
    "            sr_image = self.reconstruct_image(sr_patches, original_size, padded_size)\n",
    "            \n",
    "            # Postprocess\n",
    "            final_image = self.postprocess_image(sr_image)\n",
    "            \n",
    "            # Calculate metrics if HR image is provided\n",
    "            metrics = None\n",
    "            if hr_image_path:\n",
    "                hr_image = np.array(Image.open(hr_image_path).convert('RGB'))\n",
    "                metrics = self.evaluate_image_quality(final_image, hr_image)\n",
    "                print(\"\\nImage Quality Metrics:\")\n",
    "                print(f\"PSNR: {metrics['psnr']['average']:.2f} dB\")\n",
    "                print(f\"SSIM: {metrics['ssim']['average']:.4f}\")\n",
    "            \n",
    "            if output_path:\n",
    "                # Ensure valid file extension\n",
    "                output_path = self._ensure_valid_extension(output_path)\n",
    "                # Convert numpy array to PIL Image and save\n",
    "                Image.fromarray(final_image).save(output_path)\n",
    "                print(f\"Saved super-resolved image to {output_path}\")\n",
    "            \n",
    "            return final_image, metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during upscaling: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _ensure_valid_extension(self, filepath):\n",
    "        \"\"\"Ensure the filepath has a valid image extension\"\"\"\n",
    "        valid_extensions = {'.png', '.jpg', '.jpeg', '.bmp', '.tiff'}\n",
    "        _, ext = os.path.splitext(filepath)\n",
    "        ext = ext.lower()\n",
    "        \n",
    "        if not ext:\n",
    "            # If no extension, default to PNG\n",
    "            filepath = f\"{filepath}.png\"\n",
    "        elif ext not in valid_extensions:\n",
    "            # If invalid extension, append PNG\n",
    "            filepath = f\"{filepath}.png\"\n",
    "            \n",
    "        return filepath\n",
    "\n",
    "    def upscale_image(self, image_path, output_path=None, hr_image_path=None):\n",
    "        \"\"\"Upscale a single image and optionally calculate quality metrics\"\"\"\n",
    "        try:\n",
    "            # Preprocess\n",
    "            img_yuv, original_size = self.preprocess_image(image_path)\n",
    "            \n",
    "            # Create patches\n",
    "            patches, padded_size = self.create_patches(img_yuv)\n",
    "            \n",
    "            # Process patches\n",
    "            sr_patches = self.model.predict(patches, verbose=0)\n",
    "            \n",
    "            # Reconstruct image\n",
    "            sr_image = self.reconstruct_image(sr_patches, original_size, padded_size)\n",
    "            \n",
    "            # Postprocess\n",
    "            final_image = self.postprocess_image(sr_image)\n",
    "            \n",
    "            # Calculate metrics if HR image is provided\n",
    "            metrics = None\n",
    "            if hr_image_path:\n",
    "                hr_image = np.array(Image.open(hr_image_path).convert('RGB'))\n",
    "                metrics = self.evaluate_image_quality(final_image, hr_image)\n",
    "                print(\"\\nImage Quality Metrics:\")\n",
    "                print(f\"PSNR: {metrics['psnr']['average']:.2f} dB\")\n",
    "                print(f\"SSIM: {metrics['ssim']['average']:.4f}\")\n",
    "            \n",
    "            if output_path:\n",
    "                # Ensure valid file extension\n",
    "                output_path = self._ensure_valid_extension(output_path)\n",
    "                # Convert numpy array to PIL Image and save\n",
    "                Image.fromarray(final_image).save(output_path)\n",
    "                print(f\"Saved super-resolved image to {output_path}\")\n",
    "            \n",
    "            return final_image, metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during upscaling: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def upscale_directory(self, input_dir, output_dir, hr_dir=None):\n",
    "        \"\"\"Upscale all images in a directory and calculate metrics if HR images are available\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        metrics_summary = []\n",
    "        \n",
    "        for filename in os.listdir(input_dir):\n",
    "            base, ext = os.path.splitext(filename)\n",
    "            if ext.lower() in {'.png', '.jpg', '.jpeg', '.bmp', '.tiff'}:\n",
    "                input_path = os.path.join(input_dir, filename)\n",
    "                output_path = os.path.join(output_dir, f\"sr_{base}.png\")  # Always save as PNG\n",
    "                \n",
    "                # Check for corresponding HR image\n",
    "                hr_path = None\n",
    "                if hr_dir:\n",
    "                    # Look for HR image with any valid extension\n",
    "                    hr_base = os.path.join(hr_dir, base)\n",
    "                    for hr_ext in ['.png', '.jpg', '.jpeg', '.bmp', '.tiff']:\n",
    "                        potential_hr_path = hr_base + hr_ext\n",
    "                        if os.path.exists(potential_hr_path):\n",
    "                            hr_path = potential_hr_path\n",
    "                            break\n",
    "                    \n",
    "                    if not hr_path:\n",
    "                        print(f\"Warning: No HR image found for {filename}\")\n",
    "                \n",
    "                try:\n",
    "                    _, metrics = self.upscale_image(input_path, output_path, hr_path)\n",
    "                    if metrics:\n",
    "                        metrics_summary.append({\n",
    "                            'filename': filename,\n",
    "                            'psnr': metrics['psnr']['average'],\n",
    "                            'ssim': metrics['ssim']['average']\n",
    "                        })\n",
    "                    print(f\"Processed {filename}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filename}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        # Calculate and print average metrics\n",
    "        if metrics_summary:\n",
    "            avg_psnr = np.mean([m['psnr'] for m in metrics_summary])\n",
    "            avg_ssim = np.mean([m['ssim'] for m in metrics_summary])\n",
    "            print(\"\\nDirectory Summary:\")\n",
    "            print(f\"Average PSNR: {avg_psnr:.2f} dB\")\n",
    "            print(f\"Average SSIM: {avg_ssim:.4f}\")\n",
    "            \n",
    "            # Save metrics to CSV\n",
    "            metrics_path = os.path.join(output_dir, 'metrics_summary.csv')\n",
    "            with open(metrics_path, 'w') as f:\n",
    "                f.write('filename,psnr,ssim\\n')\n",
    "                for m in metrics_summary:\n",
    "                    f.write(f\"{m['filename']},{m['psnr']:.2f},{m['ssim']:.4f}\\n\")\n",
    "            print(f\"\\nMetrics summary saved to {metrics_path}\")\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# Initialize model\n",
    "inference_model = ESPCNInference(scale_size=2, patch_size=10)\n",
    "\n",
    "# Load trained weights\n",
    "inference_model.load_model('espcn_model.h5')\n",
    "\n",
    "# Upscale a single image and calculate metrics\n",
    "upscaled_image, metrics = inference_model.upscale_image(\n",
    "    'input.jpg',\n",
    "    'output',  # Will automatically add .png extension\n",
    "    'ground_truth_hr.jpg'  # Optional HR image for quality assessment\n",
    ")\n",
    "\n",
    "# Upscale all images in a directory and calculate metrics\n",
    "inference_model.upscale_directory(\n",
    "    'input_dir',\n",
    "    'output_dir',\n",
    "    'hr_dir'  # Optional directory containing HR images\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000025C6D096160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91995\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image Quality Metrics:\n",
      "PSNR: 5.94 dB\n",
      "SSIM: 0.0109\n",
      "Saved super-resolved image to C:/Users/91995/Downloads/Set5/Set5/image_SRF_2/Gal.png\n"
     ]
    }
   ],
   "source": [
    "inference_model = ESPCNInference(scale_size=2, patch_size=10)\n",
    "upscaled_image, metrics = inference_model.upscale_image(\n",
    "    input_file,\n",
    "    output_file,  # Will automatically add .png extension\n",
    "    gt  # Optional HR image for quality assessment\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
