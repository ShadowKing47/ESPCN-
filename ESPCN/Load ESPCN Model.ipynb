{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Dakewe Biotech Corporation. All Rights Reserved.\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#   you may not use this file except in compliance with the License.\n",
    "#   You may obtain a copy of the License at\n",
    "#\n",
    "#       http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "import os\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from natsort import natsorted\n",
    "\n",
    "# ============================== MODEL DEFINITION ==============================\n",
    "class ESPCN(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            out_channels: int,\n",
    "            channels: int,\n",
    "            upscale_factor: int,\n",
    "    ) -> None:\n",
    "        super(ESPCN, self).__init__()\n",
    "        hidden_channels = channels // 2\n",
    "        out_channels = int(out_channels * (upscale_factor ** 2))\n",
    "        # Feature mapping\n",
    "        self.feature_maps = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, channels, (5, 5), (1, 1), (2, 2)),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(channels, hidden_channels, (3, 3), (1, 1), (1, 1)),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        # Sub-pixel convolution layer\n",
    "        self.sub_pixel = nn.Sequential(\n",
    "            nn.Conv2d(hidden_channels, out_channels, (3, 3), (1, 1), (1, 1)),\n",
    "            nn.PixelShuffle(upscale_factor),\n",
    "        )\n",
    "        # Initial model weights\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                if module.in_channels == 32:\n",
    "                    nn.init.normal_(module.weight.data,\n",
    "                                    0.0,\n",
    "                                    0.001)\n",
    "                    nn.init.zeros_(module.bias.data)\n",
    "                else:\n",
    "                    nn.init.normal_(module.weight.data,\n",
    "                                    0.0,\n",
    "                                    math.sqrt(2 / (module.out_channels * module.weight.data[0][0].numel())))\n",
    "                    nn.init.zeros_(module.bias.data)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "    # Support torch.script function.\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        x = self.feature_maps(x)\n",
    "        x = self.sub_pixel(x)\n",
    "        x = torch.clamp_(x, 0.0, 1.0)\n",
    "        return x\n",
    "\n",
    "def espcn_x2(**kwargs) -> ESPCN:\n",
    "    model = ESPCN(upscale_factor=2, **kwargs)\n",
    "    return model\n",
    "\n",
    "def espcn_x3(**kwargs) -> ESPCN:\n",
    "    model = ESPCN(upscale_factor=3, **kwargs)\n",
    "    return model\n",
    "\n",
    "def espcn_x4(**kwargs) -> ESPCN:\n",
    "    model = ESPCN(upscale_factor=4, **kwargs)\n",
    "    return model\n",
    "\n",
    "def espcn_x8(**kwargs) -> ESPCN:\n",
    "    model = ESPCN(upscale_factor=8, **kwargs)\n",
    "    return model\n",
    "\n",
    "# Define available models\n",
    "model_dict = {\n",
    "    \"espcn_x2\": espcn_x2,\n",
    "    \"espcn_x3\": espcn_x3,\n",
    "    \"espcn_x4\": espcn_x4,\n",
    "    \"espcn_x8\": espcn_x8\n",
    "}\n",
    "\n",
    "# ============================== IMAGE PROCESSING FUNCTIONS ==============================\n",
    "def make_directory(dir_path):\n",
    "    \"\"\"Create directory.\n",
    "\n",
    "    Args:\n",
    "        dir_path (str): Directory path to create.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "def preprocess_one_image(image_path, device):\n",
    "    \"\"\"Preprocess a single image and convert it to a tensor.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Image file path.\n",
    "        device (torch.device): Device to use.\n",
    "\n",
    "    Returns:\n",
    "        y_tensor (torch.Tensor): Y channel tensor.\n",
    "        cb_image (numpy.ndarray): CB channel image.\n",
    "        cr_image (numpy.ndarray): CR channel image.\n",
    "    \"\"\"\n",
    "    # Read image\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    \n",
    "    # Convert BGR to YCbCr\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)\n",
    "    \n",
    "    # Split Y, Cb, and Cr channels\n",
    "    y_image, cb_image, cr_image = cv2.split(image)\n",
    "    \n",
    "    # Normalize Y channel to [0, 1]\n",
    "    y_image = y_image.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Reshape Y channel for model input\n",
    "    y_tensor = torch.from_numpy(y_image).to(device)\n",
    "    y_tensor = y_tensor.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    return y_tensor, cb_image, cr_image\n",
    "\n",
    "def tensor_to_image(tensor, range_norm=False, half=False):\n",
    "    \"\"\"Convert tensor to image.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): Tensor to convert.\n",
    "        range_norm (bool): Whether to normalize to [0, 1].\n",
    "        half (bool): Whether the tensor is in half precision.\n",
    "\n",
    "    Returns:\n",
    "        image (numpy.ndarray): Converted image.\n",
    "    \"\"\"\n",
    "    if range_norm:\n",
    "        tensor = tensor.detach().cpu().float().clamp_(0, 1)\n",
    "    if half:\n",
    "        tensor = tensor.detach().cpu().float()\n",
    "    \n",
    "    tensor = tensor.squeeze(0).squeeze(0).numpy()\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "def ycbcr_to_bgr(image):\n",
    "    \"\"\"Convert YCbCr image to BGR format.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): YCbCr image.\n",
    "\n",
    "    Returns:\n",
    "        bgr_image (numpy.ndarray): BGR image.\n",
    "    \"\"\"\n",
    "    # Convert YCbCr to BGR\n",
    "    image = image.astype(np.float32)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_YCrCb2BGR)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# ============================== IMAGE QUALITY ASSESSMENT ==============================\n",
    "class PSNR(nn.Module):\n",
    "    def __init__(self, upscale_factor=4, only_test_y_channel=True):\n",
    "        super(PSNR, self).__init__()\n",
    "        self.upscale_factor = upscale_factor\n",
    "        self.only_test_y_channel = only_test_y_channel\n",
    "\n",
    "    def forward(self, sr_tensor, hr_tensor):\n",
    "        # Don't need to crop, already done during preprocessing\n",
    "        mse = torch.mean((sr_tensor - hr_tensor) ** 2)\n",
    "        return 10. * torch.log10(1. / mse)\n",
    "\n",
    "class SSIM(nn.Module):\n",
    "    def __init__(self, upscale_factor=4, only_test_y_channel=True):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.upscale_factor = upscale_factor\n",
    "        self.only_test_y_channel = only_test_y_channel\n",
    "        self.window_size = 11\n",
    "        self.size_average = True\n",
    "        self.channel = 1\n",
    "        self.sigma = 1.5\n",
    "        self.register_buffer(\"window\", self._create_window(self.window_size, self.channel, self.sigma))\n",
    "\n",
    "    def _create_window(self, window_size, channel, sigma):\n",
    "        \"\"\"Create Gaussian window for SSIM calculation\n",
    "        \"\"\"\n",
    "        _1D_window = self._gaussian(window_size, sigma).unsqueeze(1)\n",
    "        _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "        window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "        return window\n",
    "\n",
    "    @staticmethod\n",
    "    def _gaussian(window_size, sigma):\n",
    "        gauss = torch.Tensor([math.exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])\n",
    "        return gauss / gauss.sum()\n",
    "\n",
    "    def forward(self, sr_tensor, hr_tensor):\n",
    "        # Don't need to crop, already done during preprocessing\n",
    "        c1 = (0.01 ** 2)\n",
    "        c2 = (0.03 ** 2)\n",
    "\n",
    "        window = self.window\n",
    "        mu1 = nn.functional.conv2d(sr_tensor, window, padding=self.window_size // 2, groups=self.channel)\n",
    "        mu2 = nn.functional.conv2d(hr_tensor, window, padding=self.window_size // 2, groups=self.channel)\n",
    "\n",
    "        mu1_sq = mu1.pow(2)\n",
    "        mu2_sq = mu2.pow(2)\n",
    "        mu1_mu2 = mu1 * mu2\n",
    "\n",
    "        sigma1_sq = nn.functional.conv2d(sr_tensor * sr_tensor, window, padding=self.window_size // 2, groups=self.channel) - mu1_sq\n",
    "        sigma2_sq = nn.functional.conv2d(hr_tensor * hr_tensor, window, padding=self.window_size // 2, groups=self.channel) - mu2_sq\n",
    "        sigma12 = nn.functional.conv2d(sr_tensor * hr_tensor, window, padding=self.window_size // 2, groups=self.channel) - mu1_mu2\n",
    "\n",
    "        ssim_map = ((2 * mu1_mu2 + c1) * (2 * sigma12 + c2)) / ((mu1_sq + mu2_sq + c1) * (sigma1_sq + sigma2_sq + c2))\n",
    "\n",
    "        if self.size_average:\n",
    "            return ssim_map.mean()\n",
    "        else:\n",
    "            return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================== MAIN FUNCTION ==============================\n",
    "\n",
    "def preprocess_one_image(image_path, device):\n",
    "    \"\"\"Preprocess a single image and convert it to a tensor.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Image file path.\n",
    "        device (torch.device): Device to use.\n",
    "\n",
    "    Returns:\n",
    "        rgb_tensor (torch.Tensor): RGB tensor ready for processing.\n",
    "        original_image (numpy.ndarray): Original RGB image.\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
    "        \n",
    "    # Read image in BGR (OpenCV default)\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    \n",
    "    # Check if image was loaded successfully\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Failed to load image: {image_path}\")\n",
    "    \n",
    "    # Convert BGR to RGB (OpenCV loads as BGR)\n",
    "    original_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Save a copy of the original image for debugging\n",
    "    print(f\"Input image shape: {original_image.shape}, min: {original_image.min()}, max: {original_image.max()}\")\n",
    "    \n",
    "    # Normalize RGB channels to [0, 1]\n",
    "    rgb_image = original_image.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Transpose image from (H, W, C) to (C, H, W) for PyTorch\n",
    "    rgb_image = np.transpose(rgb_image, (2, 0, 1))\n",
    "    \n",
    "    # Convert to tensor\n",
    "    rgb_tensor = torch.from_numpy(rgb_image).to(device)\n",
    "    rgb_tensor = rgb_tensor.unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    # Debug tensor values\n",
    "    print(f\"Input tensor shape: {rgb_tensor.shape}, min: {rgb_tensor.min().item()}, max: {rgb_tensor.max().item()}\")\n",
    "    \n",
    "    return rgb_tensor, original_image\n",
    "\n",
    "def main() -> None:\n",
    "    # HARDCODED PATHS - Change these to your specific locations\n",
    "    model_weights_path = r\" \"\n",
    "    lr_dir = r\" \"\n",
    "    sr_dir = r\" \"\n",
    "    gt_dir = r\" \"\n",
    "    model_arch_name = \"espcn_x4\"  # Choose from: espcn_x2, espcn_x3, espcn_x4, espcn_x8\n",
    "    \n",
    "    # IMPORTANT: Using original parameters for now since the model was trained for Y-channel\n",
    "    # We'll use a hybrid approach - process each channel separately\n",
    "    in_channels = 1\n",
    "    out_channels = 1\n",
    "    channels = 64\n",
    "    upscale_factor = 4\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    only_test_y_channel = True  # Keep this true for evaluation with the original model\n",
    "    \n",
    "    # Print selected parameters\n",
    "    print(f\"Model architecture: {model_arch_name}\")\n",
    "    print(f\"Model weights: {model_weights_path}\")\n",
    "    print(f\"Input directory: {lr_dir}\")\n",
    "    print(f\"Output directory: {sr_dir}\")\n",
    "    print(f\"Ground truth directory: {gt_dir}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Upscale factor: {upscale_factor}\")\n",
    "    print(f\"Processing in RGB mode with channel-by-channel approach\")\n",
    "\n",
    "    # Verify model weights exist\n",
    "    if not os.path.exists(model_weights_path):\n",
    "        raise FileNotFoundError(f\"Model weights not found: {model_weights_path}\")\n",
    "        \n",
    "    # Verify input directory/file exists\n",
    "    if not os.path.exists(lr_dir):\n",
    "        raise FileNotFoundError(f\"Input directory/file not found: {lr_dir}\")\n",
    "\n",
    "    # Initialize the super-resolution model\n",
    "    g_model = model_dict[model_arch_name](\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels,\n",
    "        channels=channels\n",
    "    )\n",
    "    g_model = g_model.to(device=device)\n",
    "    print(f\"Build `{model_arch_name}` model successfully.\")\n",
    "\n",
    "    # Load the super-resolution model weights\n",
    "    checkpoint = torch.load(model_weights_path, map_location=lambda storage, loc: storage)\n",
    "    g_model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    print(f\"Load `{model_arch_name}` model weights \"\n",
    "          f\"`{os.path.abspath(model_weights_path)}` successfully.\")\n",
    "\n",
    "    # Create a folder of super-resolution experiment results\n",
    "    make_directory(sr_dir)\n",
    "\n",
    "    # Start the verification mode of the model.\n",
    "    g_model.eval()\n",
    "\n",
    "    # Initialize the sharpness evaluation function for Y channel only\n",
    "    psnr = PSNR(upscale_factor, only_test_y_channel)\n",
    "    ssim = SSIM(upscale_factor, only_test_y_channel)\n",
    "\n",
    "    # Set the sharpness evaluation function calculation device to the specified model\n",
    "    psnr = psnr.to(device=device, non_blocking=True)\n",
    "    ssim = ssim.to(device=device, non_blocking=True)\n",
    "\n",
    "    # Initialize IQA metrics\n",
    "    psnr_metrics = 0.0\n",
    "    ssim_metrics = 0.0\n",
    "\n",
    "    # Check if input is a directory or single file\n",
    "    if os.path.isdir(lr_dir):\n",
    "        # Get a list of test image file names.\n",
    "        file_names = natsorted(os.listdir(lr_dir))\n",
    "        # Get the number of test image files.\n",
    "        total_files = len(file_names)\n",
    "        \n",
    "        # If no files found\n",
    "        if total_files == 0:\n",
    "            raise ValueError(f\"No files found in input directory: {lr_dir}\")\n",
    "\n",
    "        for index in range(total_files):\n",
    "            lr_image_path = os.path.join(lr_dir, file_names[index])\n",
    "            sr_image_path = os.path.join(sr_dir, file_names[index])\n",
    "            gt_image_path = os.path.join(gt_dir, file_names[index])\n",
    "            \n",
    "            # Skip non-image files (optional)\n",
    "            if not lr_image_path.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "                print(f\"Skipping non-image file: {lr_image_path}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing `{os.path.abspath(lr_image_path)}`...\")\n",
    "            \n",
    "            try:\n",
    "                # Read input image directly with OpenCV\n",
    "                lr_image = cv2.imread(lr_image_path, cv2.IMREAD_COLOR)\n",
    "                if lr_image is None:\n",
    "                    raise ValueError(f\"Failed to load image: {lr_image_path}\")\n",
    "                \n",
    "                # Convert BGR to RGB\n",
    "                lr_image_rgb = cv2.cvtColor(lr_image, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Save original dimension for reference\n",
    "                original_h, original_w = lr_image_rgb.shape[:2]\n",
    "                \n",
    "                # Initialize array for super-resolved image\n",
    "                sr_image_rgb = np.zeros((original_h * upscale_factor, original_w * upscale_factor, 3), dtype=np.uint8)\n",
    "                \n",
    "                # Process each channel separately\n",
    "                for c in range(3):  # R, G, B channels\n",
    "                    # Extract single channel and normalize\n",
    "                    channel = lr_image_rgb[:, :, c].astype(np.float32) / 255.0\n",
    "                    \n",
    "                    # Convert to tensor\n",
    "                    channel_tensor = torch.from_numpy(channel).to(device)\n",
    "                    channel_tensor = channel_tensor.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
    "                    \n",
    "                    # Process with model\n",
    "                    with torch.no_grad():\n",
    "                        sr_channel_tensor = g_model(channel_tensor)\n",
    "                    \n",
    "                    # Convert back to numpy and denormalize\n",
    "                    sr_channel = sr_channel_tensor.squeeze().cpu().detach().numpy()\n",
    "                    sr_channel = np.clip(sr_channel * 255.0, 0, 255).astype(np.uint8)\n",
    "                    \n",
    "                    # Store in the combined RGB image\n",
    "                    sr_image_rgb[:, :, c] = sr_channel\n",
    "                \n",
    "                # Save the super-resolved RGB image (convert to BGR for OpenCV)\n",
    "                sr_image_bgr = cv2.cvtColor(sr_image_rgb, cv2.COLOR_RGB2BGR)\n",
    "                cv2.imwrite(sr_image_path, sr_image_bgr)\n",
    "                \n",
    "                # Debug output\n",
    "                print(f\"SR image saved with shape: {sr_image_rgb.shape}, min: {sr_image_rgb.min()}, max: {sr_image_rgb.max()}\")\n",
    "                \n",
    "                # Process ground truth for evaluation if available\n",
    "                if os.path.exists(gt_image_path):\n",
    "                    # For proper evaluation we'd need to extract Y channel and compare\n",
    "                    gt_image = cv2.imread(gt_image_path, cv2.IMREAD_COLOR)\n",
    "                    gt_image_ycrcb = cv2.cvtColor(gt_image, cv2.COLOR_BGR2YCrCb)\n",
    "                    sr_image_ycrcb = cv2.cvtColor(sr_image_bgr, cv2.COLOR_BGR2YCrCb)\n",
    "                    \n",
    "                    # Extract Y channels for evaluation\n",
    "                    gt_y = gt_image_ycrcb[:, :, 0].astype(np.float32) / 255.0\n",
    "                    sr_y = sr_image_ycrcb[:, :, 0].astype(np.float32) / 255.0\n",
    "                    \n",
    "                    # Resize ground truth to match SR if needed\n",
    "                    if gt_y.shape != sr_y.shape:\n",
    "                        gt_y = cv2.resize(gt_y, (sr_y.shape[1], sr_y.shape[0]))\n",
    "                    \n",
    "                    # Convert to tensors for evaluation\n",
    "                    gt_y_tensor = torch.from_numpy(gt_y).to(device).unsqueeze(0).unsqueeze(0)\n",
    "                    sr_y_tensor = torch.from_numpy(sr_y).to(device).unsqueeze(0).unsqueeze(0)\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    psnr_value = psnr(sr_y_tensor, gt_y_tensor).item()\n",
    "                    ssim_value = ssim(sr_y_tensor, gt_y_tensor).item()\n",
    "                    \n",
    "                    print(f\"PSNR: {psnr_value:4.2f} [dB]\")\n",
    "                    print(f\"SSIM: {ssim_value:4.4f} [u]\")\n",
    "                    \n",
    "                    psnr_metrics += psnr_value\n",
    "                    ssim_metrics += ssim_value\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {lr_image_path}: {str(e)}\")\n",
    "                traceback.print_exc()  # Print full traceback\n",
    "                continue\n",
    "\n",
    "        # Calculate the average value of the sharpness evaluation index if any images were processed\n",
    "        if total_files > 0 and psnr_metrics > 0:\n",
    "            # PSNR range value is 0~100\n",
    "            # SSIM range value is 0~1\n",
    "            avg_psnr = 100 if psnr_metrics / total_files > 100 else psnr_metrics / total_files\n",
    "            avg_ssim = 1 if ssim_metrics / total_files > 1 else ssim_metrics / total_files\n",
    "\n",
    "            print(f\"Average PSNR: {avg_psnr:4.2f} [dB]\\n\"\n",
    "                  f\"Average SSIM: {avg_ssim:4.4f} [u]\")\n",
    "    else:\n",
    "        # Handle single file processing\n",
    "        lr_image_path = lr_dir\n",
    "        filename = os.path.basename(lr_image_path)\n",
    "        sr_image_path = os.path.join(sr_dir, filename)\n",
    "        \n",
    "        print(f\"Processing single image `{os.path.abspath(lr_image_path)}`...\")\n",
    "        \n",
    "        try:\n",
    "            # Read input image directly with OpenCV\n",
    "            lr_image = cv2.imread(lr_image_path, cv2.IMREAD_COLOR)\n",
    "            if lr_image is None:\n",
    "                raise ValueError(f\"Failed to load image: {lr_image_path}\")\n",
    "            \n",
    "            # Convert BGR to RGB\n",
    "            lr_image_rgb = cv2.cvtColor(lr_image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Save original dimension for reference\n",
    "            original_h, original_w = lr_image_rgb.shape[:2]\n",
    "            \n",
    "            # Initialize array for super-resolved image\n",
    "            sr_image_rgb = np.zeros((original_h * upscale_factor, original_w * upscale_factor, 3), dtype=np.uint8)\n",
    "            \n",
    "            # Process each channel separately\n",
    "            for c in range(3):  # R, G, B channels\n",
    "                # Extract single channel and normalize\n",
    "                channel = lr_image_rgb[:, :, c].astype(np.float32) / 255.0\n",
    "                \n",
    "                # Convert to tensor\n",
    "                channel_tensor = torch.from_numpy(channel).to(device)\n",
    "                channel_tensor = channel_tensor.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
    "                \n",
    "                # Process with model\n",
    "                with torch.no_grad():\n",
    "                    sr_channel_tensor = g_model(channel_tensor)\n",
    "                \n",
    "                # Convert back to numpy and denormalize\n",
    "                sr_channel = sr_channel_tensor.squeeze().cpu().detach().numpy()\n",
    "                sr_channel = np.clip(sr_channel * 255.0, 0, 255).astype(np.uint8)\n",
    "                \n",
    "                # Store in the combined RGB image\n",
    "                sr_image_rgb[:, :, c] = sr_channel\n",
    "            \n",
    "            # Save the super-resolved RGB image (convert to BGR for OpenCV)\n",
    "            sr_image_bgr = cv2.cvtColor(sr_image_rgb, cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite(sr_image_path, sr_image_bgr)\n",
    "            \n",
    "            print(f\"Super-resolution image saved to `{os.path.abspath(sr_image_path)}`\")\n",
    "            \n",
    "            # Check if ground truth is available\n",
    "            gt_image_path = os.path.join(gt_dir, filename)\n",
    "            if os.path.exists(gt_image_path):\n",
    "                # For proper evaluation we'd need to extract Y channel and compare\n",
    "                gt_image = cv2.imread(gt_image_path, cv2.IMREAD_COLOR)\n",
    "                gt_image_ycrcb = cv2.cvtColor(gt_image, cv2.COLOR_BGR2YCrCb)\n",
    "                sr_image_ycrcb = cv2.cvtColor(sr_image_bgr, cv2.COLOR_BGR2YCrCb)\n",
    "                \n",
    "                # Extract Y channels for evaluation\n",
    "                gt_y = gt_image_ycrcb[:, :, 0].astype(np.float32) / 255.0\n",
    "                sr_y = sr_image_ycrcb[:, :, 0].astype(np.float32) / 255.0\n",
    "                \n",
    "                # Resize ground truth to match SR if needed\n",
    "                if gt_y.shape != sr_y.shape:\n",
    "                    gt_y = cv2.resize(gt_y, (sr_y.shape[1], sr_y.shape[0]))\n",
    "                \n",
    "                # Convert to tensors for evaluation\n",
    "                gt_y_tensor = torch.from_numpy(gt_y).to(device).unsqueeze(0).unsqueeze(0)\n",
    "                sr_y_tensor = torch.from_numpy(sr_y).to(device).unsqueeze(0).unsqueeze(0)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                psnr_value = psnr(sr_y_tensor, gt_y_tensor).item()\n",
    "                ssim_value = ssim(sr_y_tensor, gt_y_tensor).item()\n",
    "                \n",
    "                print(f\"PSNR: {psnr_value:4.2f} [dB]\")\n",
    "                print(f\"SSIM: {ssim_value:4.4f} [u]\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image: {str(e)}\")\n",
    "            traceback.print_exc()  # Print full traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def restore_ycbcr_to_rgb(input_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(input_folder, filename)\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            \n",
    "            # Read image\n",
    "            img = cv2.imread(image_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            # Convert from YUV to RGB (fixing green tint issue)\n",
    "            restored_img = cv2.cvtColor(img, cv2.COLOR_YUV2RGB)\n",
    "            \n",
    "            # Save the restored image\n",
    "            cv2.imwrite(output_path, cv2.cvtColor(restored_img, cv2.COLOR_RGB2BGR))\n",
    "            print(f\"Restored: {filename} -> {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_folder = r\" \"  # Change this to your folder path\n",
    "output_folder = r\" \"\n",
    "restore_ycbcr_to_rgb(input_folder, output_folder)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
